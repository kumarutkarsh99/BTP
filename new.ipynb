{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b131b5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF cache set to: ./hf_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dt/adaptivfloat/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.9.0+cu128\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. SETUP CACHE (Prevents Permission Errors) ---\n",
    "# Forces Hugging Face to use a local directory you have write access to\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"./hf_cache\"\n",
    "os.environ[\"MPLCONFIGDIR\"] = \"./mpl_cache\"\n",
    "\n",
    "os.makedirs(\"./hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"./mpl_cache\", exist_ok=True)\n",
    "\n",
    "print(\"HF cache set to:\", os.environ[\"HF_HOME\"])\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune  # Added for Structured Pruning\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "import importlib\n",
    "\n",
    "# --- NEW IMPORTS FOR COMPRESSION ---\n",
    "import gzip           # For Entropy Coding (File Size Fix)\n",
    "import pickle         # For saving model artifacts\n",
    "import scipy.sparse   # For sparse matrix operations\n",
    "\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "\n",
    "# Load Custom Modules\n",
    "import quantizer\n",
    "import utils\n",
    "importlib.reload(utils) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33c7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REPLACEMENT FUNCTIONS FOR SAVING ---\n",
    "\n",
    "def pack_int4(int8_tensor):\n",
    "    \"\"\"\n",
    "    Packs integer values (-8 to 7) into 4-bit containers.\n",
    "    CORRECTED ORDER: Index 0 -> High Nibble, Index 1 -> Low Nibble.\n",
    "    \"\"\"\n",
    "    data = int8_tensor.flatten()\n",
    "    \n",
    "    # Shift range: -8..7 -> 0..15\n",
    "    u4 = (data + 8).astype(np.uint8)\n",
    "    \n",
    "    # Pad if odd length\n",
    "    if len(u4) % 2 != 0:\n",
    "        u4 = np.append(u4, 8) # Pad with 0 (which maps to 8)\n",
    "        \n",
    "    # Correct Pairing\n",
    "    high = u4[0::2] # Index 0, 2, 4...\n",
    "    low = u4[1::2]  # Index 1, 3, 5...\n",
    "    \n",
    "    # Pack: High << 4 | Low\n",
    "    packed = (high << 4) | low\n",
    "    return packed\n",
    "\n",
    "def save_compressed_model(model, profile, filename):\n",
    "    if not filename.endswith(\".gz\"): filename += \".gz\"\n",
    "    print(f\"ðŸ“¦ Saving with Percentile-Calibrated Per-Row Scaling to {filename}...\")\n",
    "    \n",
    "    compressed_dict = {\n",
    "        'meta': {'version': 'v4_calibrated_row_scale'},\n",
    "        'weights': {}\n",
    "    }\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' not in name or param.dim() < 2:\n",
    "            compressed_dict['weights'][name] = param.detach().cpu().numpy().astype(np.float32)\n",
    "            continue\n",
    "\n",
    "        tensor = param.detach()\n",
    "        precision = profile.get(name, 'INT8')\n",
    "        \n",
    "        # 1. Structured Pruning (Identify Active Rows)\n",
    "        row_magnitudes = torch.sum(torch.abs(tensor), dim=1)\n",
    "        active_indices = torch.nonzero(row_magnitudes).flatten().cpu().numpy()\n",
    "        reduced_tensor = tensor[active_indices, :]\n",
    "        \n",
    "        # --- THE FIX: Use Numpy for Large Layers (Embeddings) ---\n",
    "        if precision == 'INT4':\n",
    "            # 1. Move absolute values to CPU Numpy (Numpy handles >16M elements easily)\n",
    "            abs_cpu_np = torch.abs(reduced_tensor).detach().cpu().numpy().astype(np.float32)\n",
    "            \n",
    "            # 2. Calculate 99.9th percentile using Numpy\n",
    "            limit_val = np.percentile(abs_cpu_np, 99.9)\n",
    "            \n",
    "            # 3. Move the scalar limit back to the original GPU device\n",
    "            limit = torch.tensor(limit_val, device=tensor.device, dtype=torch.float32)\n",
    "            \n",
    "            # Clip the tensor\n",
    "            reduced_tensor = torch.clamp(reduced_tensor, min=-limit, max=limit)\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        # 2. PER-ROW SCALING\n",
    "        max_val_per_row, _ = torch.max(torch.abs(reduced_tensor), dim=1, keepdim=True)\n",
    "        max_val_per_row[max_val_per_row == 0] = 1.0 \n",
    "        \n",
    "        target_max = 7.0 if precision == 'INT4' else 127.0\n",
    "        scales = max_val_per_row / target_max\n",
    "        \n",
    "        # 3. Quantize\n",
    "        quantized_int = torch.round(reduced_tensor / scales)\n",
    "        \n",
    "        min_int = -8 if precision == 'INT4' else -128\n",
    "        max_int = 7 if precision == 'INT4' else 127\n",
    "        quantized_int = torch.clamp(quantized_int, min_int, max_int)\n",
    "        \n",
    "        # 4. Pack\n",
    "        quantized_np = quantized_int.cpu().numpy().astype(np.int8)\n",
    "        packed_data = pack_int4(quantized_np) if precision == 'INT4' else quantized_np\n",
    "        fmt = 'structured_int4_rows' if precision == 'INT4' else 'structured_int8_rows'\n",
    "            \n",
    "        # 5. Save\n",
    "        compressed_dict['weights'][name] = {\n",
    "            'format': fmt,\n",
    "            'data': packed_data,\n",
    "            'row_map': active_indices.astype(np.int32),\n",
    "            'scale': scales.cpu().numpy().astype(np.float32),\n",
    "            'original_shape': tensor.shape\n",
    "        }\n",
    "\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(compressed_dict, f)\n",
    "    \n",
    "    size_mb = os.path.getsize(filename) / 1024**2\n",
    "    print(f\"   Saved Size: {size_mb:.2f} MB\")\n",
    "\n",
    "# def save_compressed_model(model, profile, filename):\n",
    "#     if not filename.endswith(\".gz\"): filename += \".gz\"\n",
    "#     print(f\"ðŸ“¦ Saving with High-Precision Per-Row Scaling to {filename}...\")\n",
    "    \n",
    "#     compressed_dict = {\n",
    "#         'meta': {'version': 'v3_per_row_scale'},\n",
    "#         'weights': {}\n",
    "#     }\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'weight' not in name or param.dim() < 2:\n",
    "#             compressed_dict['weights'][name] = param.detach().cpu().numpy().astype(np.float32)\n",
    "#             continue\n",
    "\n",
    "#         tensor = param.detach()\n",
    "#         precision = profile.get(name, 'INT8')\n",
    "        \n",
    "#         # 1. Structured Pruning\n",
    "#         row_magnitudes = torch.sum(torch.abs(tensor), dim=1)\n",
    "#         active_indices = torch.nonzero(row_magnitudes).flatten().cpu().numpy()\n",
    "#         reduced_tensor = tensor[active_indices, :]\n",
    "        \n",
    "#         # 2. PER-ROW SCALING (The Upgrade)\n",
    "#         # Instead of max(whole_tensor), we calculate max(each_row)\n",
    "#         # Shape: [Rows, 1]\n",
    "#         max_val_per_row, _ = torch.max(torch.abs(reduced_tensor), dim=1, keepdim=True)\n",
    "        \n",
    "#         # Avoid division by zero\n",
    "#         max_val_per_row[max_val_per_row == 0] = 1.0\n",
    "        \n",
    "#         target_max = 7.0 if precision == 'INT4' else 127.0\n",
    "#         scales = max_val_per_row / target_max\n",
    "        \n",
    "#         # 3. Quantize\n",
    "#         # Broadcasting handles the division automatically: [Rows, Cols] / [Rows, 1]\n",
    "#         quantized_int = torch.round(reduced_tensor / scales)\n",
    "        \n",
    "#         min_int = -8 if precision == 'INT4' else -128\n",
    "#         max_int = 7 if precision == 'INT4' else 127\n",
    "#         quantized_int = torch.clamp(quantized_int, min_int, max_int)\n",
    "        \n",
    "#         # 4. Pack\n",
    "#         quantized_np = quantized_int.cpu().numpy().astype(np.int8)\n",
    "#         packed_data = pack_int4(quantized_np) if precision == 'INT4' else quantized_np\n",
    "#         fmt = 'structured_int4_rows' if precision == 'INT4' else 'structured_int8_rows'\n",
    "            \n",
    "#         # 5. Save\n",
    "#         compressed_dict['weights'][name] = {\n",
    "#             'format': fmt,\n",
    "#             'data': packed_data,\n",
    "#             'row_map': active_indices.astype(np.int32),\n",
    "#             'scale': scales.cpu().numpy().astype(np.float32), # Save as VECTOR now\n",
    "#             'original_shape': tensor.shape\n",
    "#         }\n",
    "\n",
    "#     with gzip.open(filename, 'wb') as f:\n",
    "#         pickle.dump(compressed_dict, f)\n",
    "    \n",
    "#     size_mb = os.path.getsize(filename) / 1024**2\n",
    "#     print(f\"   Saved Size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa0b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. CONFIGURATION CLASS ---\n",
    "class ExperimentConfig:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", task=\"sst2\"):\n",
    "        self.model_name = model_name\n",
    "        self.task = task\n",
    "        self.batch_size = 16 \n",
    "        self.lr = 1e-5\n",
    "        \n",
    "        # --- CRITICAL SETTINGS FOR STRUCTURED PRUNING ---\n",
    "        self.pruning_amount = 0.15  # Remove 30% of rows/neurons\n",
    "        \n",
    "        self.epochs_finetune = 10   # Baseline training\n",
    "        self.epochs_recovery = 10   # INCREASED to 3 to heal \"Broken\" rows\n",
    "    \n",
    "        self.sensitivity_threshold = 0.015 \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Define labels based on task\n",
    "        if task in ['sst2', 'qnli', 'rte', 'mrpc', 'qqp']:\n",
    "            self.num_labels = 2\n",
    "        elif task in ['mnli']:\n",
    "            self.num_labels = 3\n",
    "        elif task in ['stsb']:\n",
    "            self.num_labels = 1\n",
    "        else:\n",
    "            self.num_labels = 2 \n",
    "\n",
    "# Define Quantizers globally\n",
    "quant_af4_func = quantizer.quant_af4_func\n",
    "quant_af8_func = quantizer.quant_af8_func\n",
    "\n",
    "print(\"âœ… Config Setup Complete.\")\n",
    "\n",
    "# --- 3. HELPER FUNCTIONS ---\n",
    "\n",
    "def get_single_sample(loader, device):\n",
    "    batch = next(iter(loader))\n",
    "    if isinstance(batch, dict):\n",
    "        return {k: v[0:1].to(device) for k, v in batch.items() if k != 'label'}\n",
    "    else:\n",
    "        # Some loaders return list/tuples\n",
    "        if hasattr(batch, 'items'): # Check if it's a dict-like object\n",
    "             return {k: v[0:1].to(device) for k, v in batch.items() if k != 'label'}\n",
    "        return batch[0][0:1].to(device)\n",
    "\n",
    "def prepare_data(cfg):\n",
    "    print(f\"--- [Data] Loading {cfg.task} for {cfg.model_name} ---\")\n",
    "    \n",
    "    # Use local cache to prevent download errors\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, cache_dir=\"./hf_cache\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Map GLUE tasks to their specific input column names\n",
    "    task_to_keys = {\n",
    "        \"sst2\": (\"sentence\", None),\n",
    "        \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "        \"qnli\": (\"question\", \"sentence\"),\n",
    "        \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "        \"rte\":  (\"sentence1\", \"sentence2\"),\n",
    "        \"qqp\":  (\"question1\", \"question2\"),\n",
    "    }\n",
    "    \n",
    "    key1, key2 = task_to_keys.get(cfg.task, (\"sentence\", None))\n",
    "        \n",
    "    def tokenize_fn(batch):\n",
    "        if key2 is None:\n",
    "            return tokenizer(batch[key1], padding='max_length', truncation=True, max_length=128)\n",
    "        else:\n",
    "            return tokenizer(batch[key1], batch[key2], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "    # Load from local cache\n",
    "    raw_dataset = load_dataset(\"glue\", cfg.task, cache_dir=\"./hf_cache\")\n",
    "    tokenized_dataset = raw_dataset.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    train_loader = DataLoader(tokenized_dataset['train'], batch_size=cfg.batch_size, shuffle=True)\n",
    "    \n",
    "    # MNLI validation split handling\n",
    "    if cfg.task == 'mnli':\n",
    "        val_split = 'validation_matched'\n",
    "    else:\n",
    "        val_split = 'validation'\n",
    "        \n",
    "    val_loader = DataLoader(tokenized_dataset[val_split], batch_size=cfg.batch_size)\n",
    "    \n",
    "    # Quick Loader (Use first 500 samples for speed)\n",
    "    val_dataset = val_loader.dataset\n",
    "    if len(val_dataset) > 500:\n",
    "        subset_indices = list(range(500))\n",
    "    else:\n",
    "        subset_indices = list(range(len(val_dataset)))\n",
    "        \n",
    "    quick_loader = DataLoader(Subset(val_dataset, subset_indices), batch_size=cfg.batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, quick_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f604f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. CORE PIPELINE FUNCTIONS ---\n",
    "\n",
    "def train_fp32_baseline(cfg, train_loader, val_loader):\n",
    "    print(f\"--- [Phase 0] Fine-Tuning Baseline: {cfg.model_name} ---\")\n",
    "    \n",
    "    # Use cache_dir to prevent permission errors\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model_name, \n",
    "        num_labels=cfg.num_labels,\n",
    "        cache_dir=\"./hf_cache\"\n",
    "    ).to(cfg.device)\n",
    "    \n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        \n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader) * cfg.epochs_finetune)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(cfg.epochs_finetune), desc=\"Training Epochs\"):\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(cfg.device) for k, v in batch.items()}\n",
    "\n",
    "            if 'label' in batch:\n",
    "                batch['labels'] = batch.pop('label')\n",
    "                \n",
    "            # HF models handle labels automatically in forward()\n",
    "            outputs = model(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Evaluating FP32 Baseline...\")\n",
    "    fp32_results = {}\n",
    "    \n",
    "    # Use simple accuracy check\n",
    "    acc = utils.get_accuracy(model, val_loader, cfg.device)\n",
    "    fp32_results['accuracy'] = acc\n",
    "    fp32_results['size_mb'] = utils.get_model_size_mb(model)\n",
    "    \n",
    "    # Rough Energy Estimate\n",
    "    flops = 1e9 # Placeholder\n",
    "    fp32_results['Energy_J'] = flops * 3.7e-9 \n",
    "    \n",
    "    print(f\"Baseline Acc: {fp32_results['accuracy']:.4f}\")\n",
    "    return model, fp32_results\n",
    "\n",
    "def prune_and_recover(model, train_loader, cfg):\n",
    "    print(f\"--- [Phase 1] Structured Pruning ({cfg.pruning_amount*100}%) & Recovery ---\")\n",
    "    \n",
    "    model_pruned = copy.deepcopy(model)\n",
    "    \n",
    "    # 1. Apply STRUCTURED Pruning (Removes entire rows)\n",
    "    utils.apply_structured_pruning(model_pruned, cfg.pruning_amount)\n",
    "    \n",
    "    # 2. Recovery Training (Crucial for Structured)\n",
    "    model_pruned.train()\n",
    "    optimizer = AdamW(model_pruned.parameters(), lr=cfg.lr) \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader) * cfg.epochs_recovery)\n",
    "    \n",
    "    for epoch in tqdm(range(cfg.epochs_recovery), desc=\"Recovery Epochs\"):\n",
    "        for batch in tqdm(train_loader, desc=f\"Recovery {epoch+1}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(cfg.device) for k, v in batch.items()}\n",
    "\n",
    "            if 'label' in batch:\n",
    "                batch['labels'] = batch.pop('label')\n",
    "\n",
    "            outputs = model_pruned(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "    # 3. Make Pruning Permanent (Fixes the zeros)\n",
    "    utils.make_pruning_permanent(model_pruned)\n",
    "    \n",
    "    model_pruned.eval()\n",
    "    return model_pruned\n",
    "\n",
    "def run_greedy_search(model_pruned_base, quick_loader, cfg):\n",
    "    print(\"--- [Phase 2] Sensitivity-Aware Mixed-Precision Search ---\")\n",
    "    \n",
    "    # Get all weight matrices (Linear layers)\n",
    "    layer_names = [name for name, param in model_pruned_base.named_parameters() \n",
    "                   if 'weight' in name and param.dim() > 1]\n",
    "    \n",
    "    # 1. Baseline INT8 (All layers quantized to 8-bit)\n",
    "    model_int8 = copy.deepcopy(model_pruned_base).to(cfg.device)\n",
    "    # We use the updated utils function which handles profile application\n",
    "    utils.apply_quantization_to_model(model_int8, {'default': 'INT8'}) # Apply flat INT8\n",
    "    baseline_acc = utils.get_accuracy(model_int8, quick_loader, cfg.device)\n",
    "    print(f\"   Baseline (INT8): {baseline_acc * 100:.2f}%\")\n",
    "    del model_int8\n",
    "\n",
    "    # 2. SENSITIVITY ANALYSIS (The \"S\" in SAMPQ)\n",
    "    # We test each layer: What happens if ONLY this layer is INT4?\n",
    "    print(\"   Calculating Layer Sensitivity...\")\n",
    "    sensitivity_scores = []\n",
    "    \n",
    "    for layer_name in tqdm(layer_names, desc=\"Sensitivity Analysis\", leave=False):\n",
    "        # Create a profile where everything is INT8...\n",
    "        temp_profile = {n: 'INT8' for n in layer_names}\n",
    "        # ...except this specific layer is INT4\n",
    "        temp_profile[layer_name] = 'INT4'\n",
    "        \n",
    "        test_model = copy.deepcopy(model_pruned_base).to(cfg.device)\n",
    "        utils.apply_quantization_to_model(test_model, temp_profile)\n",
    "        acc = utils.get_accuracy(test_model, quick_loader, cfg.device)\n",
    "        del test_model\n",
    "        \n",
    "        # Drop = Baseline - New_Acc (Higher drop = More sensitive)\n",
    "        drop = baseline_acc - acc\n",
    "        sensitivity_scores.append((layer_name, drop))\n",
    "\n",
    "    # 3. SORT LAYERS (Least Sensitive First)\n",
    "    # We want to quantize the \"safe\" layers first\n",
    "    sensitivity_scores.sort(key=lambda x: x[1]) \n",
    "    sorted_layers = [x[0] for x in sensitivity_scores]\n",
    "    \n",
    "    print(f\"   Most Robust Layer: {sorted_layers[0]} (Drop: {sensitivity_scores[0][1]:.4f})\")\n",
    "    print(f\"   Most Sensitive Layer: {sorted_layers[-1]} (Drop: {sensitivity_scores[-1][1]:.4f})\")\n",
    "\n",
    "    # 4. GREEDY OPTIMIZATION LOOP\n",
    "    final_profile = {layer: 'INT8' for layer in layer_names}\n",
    "    \n",
    "    # We allow accuracy to drop by 'sensitivity_threshold' total\n",
    "    current_acc_limit = baseline_acc - cfg.sensitivity_threshold\n",
    "    \n",
    "    pbar = tqdm(sorted_layers, desc=\"Greedy Optimization\")\n",
    "    \n",
    "    for layer_name in pbar:\n",
    "        # Attempt to demote this layer to INT4\n",
    "        final_profile[layer_name] = 'INT4'\n",
    "        \n",
    "        # Test the cumulative effect\n",
    "        test_model = copy.deepcopy(model_pruned_base).to(cfg.device)\n",
    "        utils.apply_quantization_to_model(test_model, final_profile)\n",
    "        acc = utils.get_accuracy(test_model, quick_loader, cfg.device)\n",
    "        del test_model\n",
    "        \n",
    "        # If accuracy drops too much below our limit, REVERT it\n",
    "        if acc < current_acc_limit:\n",
    "            final_profile[layer_name] = 'INT8' # Revert\n",
    "        else:\n",
    "            # Keep INT4\n",
    "            pass\n",
    "            \n",
    "        pbar.set_postfix({'Acc': f\"{acc:.4f}\", 'INT4_Count': list(final_profile.values()).count('INT4')})\n",
    "\n",
    "    return final_profile\n",
    "\n",
    "def build_and_evaluate_final(model_pruned, profile, val_loader, cfg):\n",
    "    print(\"--- [Phase 3] Building & Evaluating Final Hybrid Model ---\")\n",
    "    \n",
    "    # We work on the pruned model (which is already permanent)\n",
    "    model_hybrid = copy.deepcopy(model_pruned)\n",
    "    \n",
    "    # Apply Quantization Profile (Fake Quant for eval)\n",
    "    utils.apply_quantization_to_model(model_hybrid, profile)\n",
    "    \n",
    "    # Evaluate\n",
    "    acc = utils.get_accuracy(model_hybrid, val_loader, cfg.device)\n",
    "    \n",
    "    # Metrics\n",
    "    results = {}\n",
    "    results['accuracy'] = acc\n",
    "    results['sparsity'] = cfg.pruning_amount * 100\n",
    "    \n",
    "    int4_c = list(profile.values()).count('INT4')\n",
    "    int8_c = list(profile.values()).count('INT8')\n",
    "    avg_bits = ((int4_c * 4) + (int8_c * 8)) / (int4_c + int8_c) if (int4_c + int8_c) > 0 else 8.0\n",
    "    \n",
    "    # Theoretical Size Calculation\n",
    "    # Structured pruning reduces parameter count, so we use current numel()\n",
    "    total_params = sum(p.numel() for p in model_hybrid.parameters())\n",
    "    hybrid_size = (total_params * avg_bits) / (8 * 1024**2)\n",
    "    \n",
    "    return {\n",
    "        \"Model\": cfg.model_name,\n",
    "        \"Hybrid_Acc\": results['accuracy'],\n",
    "        \"Sparsity\": results['sparsity'],\n",
    "        \"Avg_Bits\": avg_bits,\n",
    "        \"Hybrid_Size_MB\": hybrid_size, # Theoretical\n",
    "        \"Task\": cfg.task\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23249833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING STRUCTURED RUN: 2 Models on mnli\n",
      "\n",
      "========================================\n",
      "Running Pipeline for: gpt2 on mnli\n",
      "========================================\n",
      "--- [Data] Loading mnli for gpt2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9847/9847 [00:00<00:00, 16383.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Phase 0] Fine-Tuning Baseline: gpt2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1718.55it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2ForSequenceClassification LOAD REPORT from: gpt2\n",
      "Key                  | Status     | \n",
      "---------------------+------------+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED | \n",
      "score.weight         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Training Epochs:  20%|â–ˆâ–ˆ        | 2/10 [57:26<3:49:42, 1722.84s/it]"
     ]
    }
   ],
   "source": [
    "# Create output folder\n",
    "folder_name = \"compressed_models_structured\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# --- 5. MAIN EXECUTION LOOP ---\n",
    "\n",
    "models_to_test = [\n",
    "    # \"prajjwal1/bert-tiny\",                  \n",
    "    # \"google/electra-small-discriminator\",   \n",
    "    # \"distilbert-base-uncased\",              \n",
    "    # \"bert-base-uncased\",                    \n",
    "    # \"roberta-base\",                         \n",
    "    \"gpt2\",                                 \n",
    "    \"albert-base-v2\",                       \n",
    "]\n",
    "\n",
    "# CHANGE THIS TO \"qnli\", \"mnli\", \"mrpc\"\n",
    "target_task = \"mnli\"  \n",
    "\n",
    "paper_results = []\n",
    "\n",
    "print(f\"ðŸš€ STARTING STRUCTURED RUN: {len(models_to_test)} Models on {target_task}\")\n",
    "\n",
    "for m_name in models_to_test:\n",
    "    print(f\"\\n{'='*40}\\nRunning Pipeline for: {m_name} on {target_task}\\n{'='*40}\")\n",
    "    \n",
    "    cfg = ExperimentConfig(model_name=m_name, task=target_task)\n",
    "    \n",
    "    try:\n",
    "        # Prepare Data\n",
    "        train_loader, val_loader, quick_loader = prepare_data(cfg)\n",
    "        \n",
    "        # Phase 0: Baseline\n",
    "        model_fp32, fp32_res = train_fp32_baseline(cfg, train_loader, val_loader)\n",
    "        \n",
    "        # Phase 1: Structured Pruning (Removes Rows)\n",
    "        model_pruned = prune_and_recover(model_fp32, train_loader, cfg)\n",
    "        del model_fp32 \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Phase 2: Search (Finds Mixed Precision)\n",
    "        profile = run_greedy_search(model_pruned, quick_loader, cfg)\n",
    "        \n",
    "        # Phase 3: Final Eval\n",
    "        final_stats = build_and_evaluate_final(model_pruned, profile, val_loader, cfg)\n",
    "        \n",
    "        # --- PHASE 4: SAVE REAL COMPRESSED MODEL ---\n",
    "        \n",
    "        # 1. Prepare model for saving\n",
    "        # Note: model_pruned already has rows removed permanently.\n",
    "        # We assume the saver handles the quantization packing based on 'profile'.\n",
    "        model_to_save = copy.deepcopy(model_pruned)\n",
    "        \n",
    "        # 2. Save it\n",
    "        safe_name = m_name.replace(\"/\", \"_\")\n",
    "        filename = f\"{folder_name}/{safe_name}_{target_task}.pkl\" \n",
    "        \n",
    "        # This calls the NEW Structured Saver (from Cell 3)\n",
    "        save_compressed_model(model_to_save, profile, filename)\n",
    "        \n",
    "        del model_to_save \n",
    "        # -------------------------------------------------\n",
    "\n",
    "        # Metrics\n",
    "        final_stats['FP32_Acc'] = fp32_res['accuracy']\n",
    "        final_stats['FP32_Energy_J'] = fp32_res['Energy_J']\n",
    "        final_stats['Acc_Loss'] = fp32_res['accuracy'] - final_stats['Hybrid_Acc']\n",
    "        final_stats['Task'] = target_task\n",
    "        \n",
    "        paper_results.append(final_stats)\n",
    "        \n",
    "        # Save Checkpoint CSV\n",
    "        pd.DataFrame(paper_results).to_csv(f\"paper_results_{target_task}_checkpoint.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed on {m_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\\nðŸ“Š FINAL RESULTS ðŸ“Š\")\n",
    "df = pd.DataFrame(paper_results)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "print(df)\n",
    "df.to_csv(f\"research_paper_final_results_{target_task}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
