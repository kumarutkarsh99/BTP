{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efc6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dt/adaptivfloat/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Loading compressed model: compressed_models_structured/google_electra-small-discriminator_mnli.pkl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 2490.39it/s, Materializing param=electra.encoder.layer.11.output.dense.weight]              \n",
      "ElectraForSequenceClassification LOAD REPORT from: google/electra-small-discriminator\n",
      "Key                                               | Status     | \n",
      "--------------------------------------------------+------------+-\n",
      "discriminator_predictions.dense_prediction.bias   | UNEXPECTED | \n",
      "discriminator_predictions.dense.weight            | UNEXPECTED | \n",
      "discriminator_predictions.dense.bias              | UNEXPECTED | \n",
      "discriminator_predictions.dense_prediction.weight | UNEXPECTED | \n",
      "classifier.dense.weight                           | MISSING    | \n",
      "classifier.out_proj.bias                          | MISSING    | \n",
      "classifier.out_proj.weight                        | MISSING    | \n",
      "classifier.dense.bias                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”“ Decompressing layers...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ElectraForSequenceClassification:\n\tsize mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 256]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n\tsize mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸŽ¯ Final QNLI Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     model = \u001b[43mload_compressed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m)\u001b[49m.to(DEVICE)\n\u001b[32m    104\u001b[39m     model.eval()\n\u001b[32m    106\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mload_compressed_model\u001b[39m\u001b[34m(file_path, base_model_name)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     87\u001b[39m         state_dict[name] = torch.from_numpy(data)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m missing, _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(k.startswith(\u001b[33m\"\u001b[39m\u001b[33mclassifier.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing):\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸš¨ CRITICAL WARNING ðŸš¨\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/adaptivfloat/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ElectraForSequenceClassification:\n\tsize mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 256]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n\tsize mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# ================= CONFIG =================\n",
    "MODEL_PATH = \"compressed_models_structured/google_electra-small-discriminator_qnli.pkl.gz\"\n",
    "BASE_MODEL = \"google/electra-small-discriminator\"   # must match compressed model\n",
    "\n",
    "TASK_NAME = \"qnli\"\n",
    "NUM_LABELS = 2\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 128\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# =========================================\n",
    "\n",
    "\n",
    "def unpack_int4_to_tensor(packed_data, original_count):\n",
    "    packed = torch.from_numpy(packed_data)\n",
    "    high = (packed >> 4).to(torch.int8)\n",
    "    low = (packed & 0x0F).to(torch.int8)\n",
    "\n",
    "    unpacked = torch.empty(len(high) * 2, dtype=torch.int8)\n",
    "    unpacked[0::2] = high\n",
    "    unpacked[1::2] = low\n",
    "    unpacked = unpacked - 8\n",
    "\n",
    "    return unpacked[:original_count].float()\n",
    "\n",
    "\n",
    "def load_compressed_model(file_path, base_model_name):\n",
    "    print(f\"\\nðŸ“¦ Loading compressed model: {file_path}\")\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=NUM_LABELS,\n",
    "        finetuning_task=TASK_NAME,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        wrapper = pickle.load(f)\n",
    "\n",
    "    weights = wrapper[\"weights\"]\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    print(\"ðŸ”“ Decompressing layers...\")\n",
    "    for name, data in weights.items():\n",
    "        if isinstance(data, dict):\n",
    "            packed = data[\"data\"]\n",
    "            scale = data[\"scale\"]\n",
    "            row_map = data[\"row_map\"]\n",
    "            orig_shape = data[\"original_shape\"]\n",
    "            fmt = data[\"format\"]\n",
    "\n",
    "            count = len(row_map) * orig_shape[1]\n",
    "\n",
    "            flat = (\n",
    "                unpack_int4_to_tensor(packed, count)\n",
    "                if \"int4\" in fmt\n",
    "                else torch.from_numpy(packed).float().flatten()\n",
    "            )\n",
    "\n",
    "            rows = flat.view(len(row_map), orig_shape[1])\n",
    "            scale = torch.tensor(scale, dtype=torch.float32)\n",
    "            full = torch.zeros(orig_shape)\n",
    "            full[row_map] = rows * scale\n",
    "            state_dict[name] = full\n",
    "        else:\n",
    "            state_dict[name] = torch.from_numpy(data)\n",
    "\n",
    "    missing, _ = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    if any(k.startswith(\"classifier.\") for k in missing):\n",
    "        print(\"\\nðŸš¨ CRITICAL WARNING ðŸš¨\")\n",
    "        print(\"Classifier head is MISSING.\")\n",
    "        print(\"You compressed a BASE model, not a QNLI-finetuned model.\")\n",
    "        print(\"Accuracy will be ~50%.\\n\")\n",
    "    else:\n",
    "        print(\"âœ… Classifier head loaded correctly.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = load_compressed_model(MODEL_PATH, BASE_MODEL).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    print(\"\\nðŸ“Š Loading GLUE/QNLI\")\n",
    "    raw = load_dataset(\"nyu-mll/glue\", TASK_NAME)\n",
    "\n",
    "    metric = evaluate.load(\"glue\", TASK_NAME)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"question\"],\n",
    "            batch[\"sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LEN,\n",
    "        )\n",
    "\n",
    "    dataset = raw[\"validation\"].map(preprocess, batched=True)\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    if \"token_type_ids\" in dataset.column_names:\n",
    "        keep_cols.append(\"token_type_ids\")\n",
    "\n",
    "    dataset.set_format(\"torch\", columns=keep_cols)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    print(\"ðŸš€ Evaluating...\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "    result = metric.compute()\n",
    "    print(f\"\\nðŸŽ¯ Final QNLI Accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
